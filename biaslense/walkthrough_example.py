#!/usr/bin/env python3
"""
BiasLens Walkthrough Example
Detailed demonstration of the complete bias detection and mitigation process
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from core.rubric_scoring import BiasRubricScorer
from core.embedding_checker import EmbeddingChecker
from core.bias_mitigator import BAMIPMitigator


def main():
    """Walk through a complete BiasLens analysis"""
    
    print("ğŸ§  BiasLens Complete Walkthrough")
    print("=" * 60)
    
    # Initialize models
    print("ğŸ“¦ Loading Models...")
    scorer = BiasRubricScorer()
    embedder = EmbeddingChecker()
    mitigator = BAMIPMitigator()
    print("âœ… All models loaded successfully!\n")
    
    # Example text with multiple bias issues
    example_text = "All Sikhs are terrorists and should be feared. They wear turbans to hide their violent nature and carry daggers as weapons. Sikhism is just a subset of Islam that promotes extremism."
    
    print("ğŸ“ INPUT TEXT:")
    print(f'"{example_text}"')
    print("\n" + "=" * 60)
    
    # STEP 1: RUBRIC SCORING
    print("\nğŸ¯ STEP 1: RUBRIC SCORING")
    print("-" * 30)
    
    rubric_result = scorer.score_text(example_text)
    
    print("ğŸ“Š Individual Dimension Scores:")
    print(f"  Accuracy: {rubric_result.accuracy_score:.1f}/10")
    print(f"  Fairness: {rubric_result.fairness_score:.1f}/10")
    print(f"  Representation: {rubric_result.representation_score:.1f}/10")
    print(f"  Linguistic Balance: {rubric_result.linguistic_balance_score:.1f}/10")
    print(f"  Cultural Framing: {rubric_result.cultural_framing_score:.1f}/10")
    
    print(f"\nğŸ¯ Overall Score: {rubric_result.overall_score:.1f}/10")
    print(f"ğŸ’¡ Confidence: {rubric_result.confidence:.1%}")
    
    # Determine severity
    if rubric_result.overall_score >= 8.5:
        severity = "LOW"
        color = "ğŸŸ¢"
    elif rubric_result.overall_score >= 6.0:
        severity = "MEDIUM"
        color = "ğŸŸ¡"
    else:
        severity = "HIGH"
        color = "ğŸ”´"
    
    print(f"\n{color} BIAS SEVERITY: {severity}")
    
    print("\nğŸ’¡ Bias Explanations:")
    for explanation in rubric_result.explanations:
        print(f"  â€¢ {explanation}")
    
    print("\nğŸš© Flagged Patterns:")
    for pattern in rubric_result.flagged_patterns:
        print(f"  â€¢ '{pattern}'")
    
    # STEP 2: SIMILARITY ANALYSIS
    print("\n\nğŸ” STEP 2: SIMILARITY ANALYSIS")
    print("-" * 30)
    
    similarity_result = embedder.compute_similarity(example_text)
    
    print(f"ğŸ“Š Max Similarity: {similarity_result.max_similarity:.3f}")
    print(f"ğŸ¯ Threshold Exceeded: {'Yes' if similarity_result.threshold_exceeded else 'No'}")
    print(f"ğŸ“ˆ Severity Level: {similarity_result.severity_level.title()}")
    
    print(f"\nğŸ” Similar Stereotype Phrases ({len(similarity_result.similar_phrases)} found):")
    for phrase in similarity_result.similar_phrases[:5]:  # Show top 5
        score = similarity_result.similarity_scores.get(phrase, 0.0)
        print(f"  â€¢ '{phrase}' (similarity: {score:.3f})")
    
    # STEP 3: BAMIP STRATEGY SELECTION
    print("\n\nğŸ› ï¸ STEP 3: BAMIP STRATEGY SELECTION")
    print("-" * 30)
    
    # Show strategy selection process
    text_lower = example_text.lower()
    
    print("ğŸ” Pattern Analysis:")
    
    # Check each pattern in priority order
    patterns_to_check = [
        (r'\b(sikh.*muslim|muslim.*sikh|sikh.*islam|islam.*sikh|sikh.*subset.*islam)\b', "Religious Conflation", "Retrieval Grounding"),
        (r'\b(terrorist|terrorism|militant|extremist)\b', "Terrorism Association", "Neutral Language"),
        (r'\b(all|every|always|never)\s+(sikh|muslim|jew|hindu)\b', "Harmful Generalizations", "Contextual Reframing"),
        (r'\b(backward|primitive|uncivilized)\b', "Cultural Bias", "Counter Narrative"),
        (r'\b(fear|danger|threat|violent|aggressive)\b', "Emotional Language", "Neutral Language"),
    ]
    
    selected_strategy = None
    for pattern, description, strategy in patterns_to_check:
        if re.search(pattern, text_lower):
            print(f"  âœ… Found: {description} â†’ {strategy}")
            if selected_strategy is None:
                selected_strategy = strategy
                print(f"  ğŸ¯ SELECTED STRATEGY: {strategy}")
        else:
            print(f"  âŒ Not found: {description}")
    
    if selected_strategy is None:
        selected_strategy = "Instructional Prompting"
        print(f"  ğŸ¯ DEFAULT STRATEGY: {selected_strategy}")
    
    # STEP 4: BAMIP MITIGATION
    print("\n\nğŸ› ï¸ STEP 4: BAMIP MITIGATION")
    print("-" * 30)
    
    mitigation_result = mitigator.mitigate_bias(example_text)
    
    print(f"ğŸ¯ Strategy Used: {mitigation_result.strategy_used.value.replace('_', ' ').title()}")
    print(f"ğŸ“‰ Bias Reduction: {mitigation_result.bias_reduction_score:.1%}")
    print(f"ğŸ’¡ Confidence: {mitigation_result.confidence:.1%}")
    
    print(f"\nğŸ“ ORIGINAL TEXT:")
    print(f'"{mitigation_result.original_text}"')
    
    print(f"\nğŸ“ MITIGATED TEXT:")
    print(f'"{mitigation_result.mitigated_text}"')
    
    print(f"\nğŸ’¡ Mitigation Strategy:")
    for explanation in mitigation_result.explanations:
        print(f"  â€¢ {explanation}")
    
    print(f"\nğŸ¯ Suggested Bias-Free Prompts:")
    for i, prompt in enumerate(mitigation_result.suggested_prompts, 1):
        print(f"  {i}. {prompt}")
    
    # STEP 5: FINAL ANALYSIS
    print("\n\nğŸ“Š STEP 5: FINAL ANALYSIS")
    print("-" * 30)
    
    print("ğŸ¯ COMPLETE ASSESSMENT:")
    print(f"  Bias Level: {color} {severity}")
    print(f"  Overall Score: {rubric_result.overall_score:.1f}/10")
    print(f"  Similarity Risk: {similarity_result.severity_level.title()}")
    print(f"  Mitigation Applied: {mitigation_result.strategy_used.value.replace('_', ' ').title()}")
    print(f"  Bias Reduced: {mitigation_result.bias_reduction_score:.1%}")
    
    print(f"\nğŸš¨ KEY ISSUES IDENTIFIED:")
    print(f"  â€¢ Factual errors about Sikhism and Islam")
    print(f"  â€¢ Harmful generalizations ('All Sikhs')")
    print(f"  â€¢ Terrorism association")
    print(f"  â€¢ Emotional language ('feared', 'violent')")
    
    print(f"\nâœ… MITIGATION ACTIONS:")
    print(f"  â€¢ Added factual context about Sikhism")
    print(f"  â€¢ Suggested bias-free prompts for future use")
    print(f"  â€¢ Provided educational explanations")
    
    print(f"\nğŸ¯ RECOMMENDATIONS:")
    print(f"  â€¢ Use suggested bias-free prompts")
    print(f"  â€¢ Apply additional Neutral Language strategy")
    print(f"  â€¢ Include more factual context about Sikhism")
    print(f"  â€¢ Avoid generalizations about religious groups")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Walkthrough Complete!")
    print("This demonstrates the complete BiasLens pipeline from detection to mitigation.")


if __name__ == "__main__":
    import re
    main() 